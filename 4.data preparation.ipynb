{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12092, 3645, 432, 4135, 2]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Hello World from China!'\n",
    "encoded_text = tokenizer(text)['input_ids']\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: Hello World from China!\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print('Decoded text:', decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded several texts: [[12092, 3645, 432, 4135, 2], [13815, 12949, 310, 1270, 2], [17491, 7192]]\n"
     ]
    }
   ],
   "source": [
    "text_ls = ['Hello World from China!', 'Mac mini is great!', 'Copy understood']\n",
    "encoded_texts = tokenizer(text_ls)\n",
    "print('Encoded several texts:', encoded_texts['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded several texts with padding: [[12092, 3645, 432, 4135, 2], [13815, 12949, 310, 1270, 2], [17491, 7192, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "encoded_texts_longest = tokenizer(text_ls, padding=True)\n",
    "print('Encoded several texts with padding:', encoded_texts_longest['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded several texts with truncation: [[12092, 3645, 432], [13815, 12949, 310], [17491, 7192]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_truncation = tokenizer(text_ls, max_length=3, truncation=True)\n",
    "print('Encoded several texts with truncation:', encoded_texts_truncation['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded several texts with left_truncation: [[432, 4135, 2], [310, 1270, 2], [17491, 7192]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.truncation_side = 'left'\n",
    "encoded_texts_truncation_left = tokenizer(text_ls, max_length=3, truncation=True)\n",
    "print('Encoded several texts with left_truncation:', encoded_texts_truncation_left['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded several texts with both padding and truncation: [[432, 4135, 2], [310, 1270, 2], [17491, 7192, 0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_both = tokenizer(text_ls, max_length=3, truncation=True, padding=True)\n",
    "print('Encoded several texts with both padding and truncation:', encoded_texts_both['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'One datapoint in the finetuning dataset:'\n",
      "{'answer': 'There are several metrics that can be used to evaluate the '\n",
      "           'performance and quality of generated text from Lamini models, '\n",
      "           'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
      "           'measures how well the model predicts the next word in a sequence, '\n",
      "           'while BLEU score measures the similarity between the generated '\n",
      "           'text and a reference text. Human evaluation involves having human '\n",
      "           'judges rate the quality of the generated text based on factors '\n",
      "           'such as coherence, fluency, and relevance. It is recommended to '\n",
      "           'use a combination of these metrics for a comprehensive evaluation '\n",
      "           \"of the model's performance.\",\n",
      " 'question': '### Question:\\n'\n",
      "             'How can I evaluate the performance and quality of the generated '\n",
      "             'text from Lamini models?\\n'\n",
      "             '\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"hf://datasets/kotzeje/lamini_docs.jsonl/data/train-00000-of-00001-6359aa989b671345.parquet\")\n",
    "examples = df.to_dict()\n",
    "\n",
    "if 'question' in examples and 'answer' in examples:\n",
    "    text = examples[\"question\"][0] + examples['answer'][0]\n",
    "elif 'instruction' in examples and 'response' in examples:\n",
    "    text = examples[\"instruction\"][0] + examples['response'][0]\n",
    "elif 'input' in examples and 'output' in examples:\n",
    "    text = examples[\"input\"][0] + examples['output'][0]\n",
    "else:\n",
    "    text = examples[\"text\"][0]\n",
    "\n",
    "prompt_template = '''### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:'''\n",
    "\n",
    "num_examples = len(examples['question'])\n",
    "finetuning_dataset = []\n",
    "for i in range(num_examples):\n",
    "    question = examples['question'][i]\n",
    "    answer = examples['answer'][i]\n",
    "    text_with_prompt_template = prompt_template.format(question=question)\n",
    "    finetuning_dataset.append({'question': text_with_prompt_template, 'answer': answer})\n",
    "\n",
    "pprint('One datapoint in the finetuning dataset:')\n",
    "pprint(finetuning_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4118 19782    27   187  2347   476   309  7472   253  3045   285  3290\n",
      "    273   253  4561  2505   432   418  4988    74  3210    32   187   187\n",
      "   4118 37741    27  2512   403  2067 17082   326   476   320   908   281\n",
      "   7472   253  3045   285  3290   273  4561  2505   432   418  4988    74\n",
      "   3210    13  1690 44229   414    13   378  1843    54  4868    13   285\n",
      "   1966  7103    15  3545 12813   414  5593   849   973   253  1566 26295\n",
      "    253  1735  3159   275   247  3425    13  1223   378  1843    54  4868\n",
      "   5593   253 14259   875   253  4561  2505   285   247  3806  2505    15\n",
      "   8801  7103  8687  1907  1966 16006  2281   253  3290   273   253  4561\n",
      "   2505  1754   327  2616   824   347 25253    13  2938  1371    13   285\n",
      "  17200    15   733   310  8521   281   897   247  5019   273   841 17082\n",
      "    323   247 11088  7103   273   253  1566   434  3045    15]]\n"
     ]
    }
   ],
   "source": [
    "text = finetuning_dataset[0]['question'] + finetuning_dataset[0]['answer']\n",
    "tokenized_inputs = tokenizer(text, return_tensors='np', \n",
    "                             padding=True, \n",
    "                             truncation=True)\n",
    "print(tokenized_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx_length = 2048\n",
    "mx_length = min(mx_length,\n",
    "                tokenized_inputs['input_ids'].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4118, 19782,    27,   187,  2347,   476,   309,  7472,   253,\n",
       "         3045,   285,  3290,   273,   253,  4561,  2505,   432,   418,\n",
       "         4988,    74,  3210,    32,   187,   187,  4118, 37741,    27,\n",
       "         2512,   403,  2067, 17082,   326,   476,   320,   908,   281,\n",
       "         7472,   253,  3045,   285,  3290,   273,  4561,  2505,   432,\n",
       "          418,  4988,    74,  3210,    13,  1690, 44229,   414,    13,\n",
       "          378,  1843,    54,  4868,    13,   285,  1966,  7103,    15,\n",
       "         3545, 12813,   414,  5593,   849,   973,   253,  1566, 26295,\n",
       "          253,  1735,  3159,   275,   247,  3425,    13,  1223,   378,\n",
       "         1843,    54,  4868,  5593,   253, 14259,   875,   253,  4561,\n",
       "         2505,   285,   247,  3806,  2505,    15,  8801,  7103,  8687,\n",
       "         1907,  1966, 16006,  2281,   253,  3290,   273,   253,  4561,\n",
       "         2505,  1754,   327,  2616,   824,   347, 25253,    13,  2938,\n",
       "         1371,    13,   285, 17200,    15,   733,   310,  8521,   281,\n",
       "          897,   247,  5019,   273,   841, 17082,   323,   247, 11088,\n",
       "         7103,   273,   253,  1566,   434,  3045,    15]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(text, return_tensors='np', \n",
    "                             padding=True,\n",
    "                             truncation=True,\n",
    "                             max_length=mx_length)\n",
    "tokenized_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapped into a function\n",
    "def tokenizer_function(examples):\n",
    "    if 'question' in examples and 'answer' in examples:\n",
    "        text = examples[\"question\"][0] + examples['answer'][0]\n",
    "    elif 'input' in examples and 'output' in examples:\n",
    "        text = examples[\"input\"][0] + examples['output'][0]\n",
    "    else:\n",
    "        text = examples[\"text\"][0]\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(text, \n",
    "                                 return_tensors='np', \n",
    "                                 truncation=True)\n",
    "    mx_length = min(2048,\n",
    "                    tokenized_inputs['input_ids'].shape[1])\n",
    "    tokenizer.truncation_side = 'left'\n",
    "    tokenized_inputs = tokenizer(text, \n",
    "                                 return_tensors='np', \n",
    "                                 truncation=True,\n",
    "                                 max_length=mx_length)\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_loaded = datasets.load_dataset('json', data_files='lamini_docs.jsonl')['train']\n",
    "tokenized_dataset = finetuning_dataset_loaded.map(tokenizer_function,\n",
    "                                                  batched=True,\n",
    "                                                  batch_size=1,\n",
    "                                                  drop_last_batch=True)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column('labels', tokenized_dataset['input_ids'])\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
