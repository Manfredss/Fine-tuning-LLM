{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config, logging, os, random, tempfile, time, yaml\n",
    "import datasets\n",
    "import lamini\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "lamini.api_key = os.getenv(\"4ebe383bfcbbfe832151b39255909ed120aa75e9e1e907d20be586c80cf7e2cd\")\n",
    "\n",
    "from lamini import Lamini\n",
    "from utilities import *\n",
    "from torch import nn, optim\n",
    "from torch import functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "global_config = None\n",
    "\n",
    "use_hf = True  # True\n",
    "dataset_path = 'lamini_docs.jsonl'\n",
    "model_name = 'EleutherAI/pythia-410m'\n",
    "training_config = {\n",
    "    'model': {\n",
    "        'pretrained_name': model_name,\n",
    "        'mx_length': 2048},\n",
    "    'datasets': {\n",
    "        'use_hf': use_hf,\n",
    "        'path': dataset_path},\n",
    "    'verbose': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba0ae55e4b4471f87bc59c439c5c582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tzy12\\.conda\\envs\\pytorch2.1.0\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tzy12\\.cache\\huggingface\\hub\\models--EleutherAI--pythia-410m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d199c07fbba4ac0b2d85f5a571e9683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1667f73597d4bb88d8c815871bf3dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1260\n",
      "}) Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 140\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "print(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de921a2175034b1798380c41a03802ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722f076118034ca18c2340f5a77c467a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/911M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 1024)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=1024, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    logger.debug('Select GPU device')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    logger.debug('Select CPU device')\n",
    "    device = torch.device('cpu')\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(text, \n",
    "                                 return_tensors='pt', \n",
    "                                 max_length=max_input_tokens, \n",
    "                                 truncation=True)\n",
    "    # Generate the output\n",
    "    device = model.device\n",
    "    generated_tokens_with_prompt = model.generate(input_ids=input_ids.to(device),\n",
    "                                                  max_length=max_output_tokens)\n",
    "    # Decode the output\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt,\n",
    "                                                        skip_special_tokens=True)\n",
    "    # Strip the prompt\n",
    "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "    return generated_text_answer\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test):  Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Correct answer from Lamini docs: Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
      "Model's answer: \n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "I think you are looking for the Lamini documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "I think you are looking for the Lamini documentation.\n",
      "\n",
      "I think you are looking for the Lamini documentation.\n",
      "\n",
      "I think you are looking for the Lamini documentation.\n",
      "\n",
      "I think you are looking for the Lamini documentation.\n",
      "\n",
      "I think you are looking\n"
     ]
    }
   ],
   "source": [
    "test_text = test_dataset[0]['question']\n",
    "print('Question input (test): ', test_text)\n",
    "print(f'Correct answer from Lamini docs: {test_dataset[0][\"answer\"]}')\n",
    "print('Model\\'s answer: ')\n",
    "print(inference(test_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tzy12\\.conda\\envs\\pytorch2.1.0\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_steps = 100\n",
    "trained_model_name = f'lamini_docs_{max_steps}_steps'\n",
    "output_dir = trained_model_name\n",
    "training_args = TrainingArguments(\n",
    "    # lr\n",
    "    learning_rate=1e-5,\n",
    "\n",
    "    # Training steps\n",
    "    num_train_epochs=1,\n",
    "\n",
    "    # Max steps to train for (each step is a batch of data)\n",
    "    # Overriddes num_train_epochs, if not -1\n",
    "    max_steps=max_steps,\n",
    "\n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size=1,\n",
    "\n",
    "    # Directory to save the checkpoints\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Other args\n",
    "    overwrite_output_dir=True,  # Overwrite the content of the output directory\n",
    "    disable_tqdm=False,  # Disable tqdm progress bars\n",
    "    eval_steps=120,  # Number of update steps between two evaluations\n",
    "    save_steps=120,  # After how many steps to save the model\n",
    "    warmup_steps=2,  # Number of warmup steps for learning rate scheduler\n",
    "    per_device_eval_batch_size=1,  # Batch size for evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=1,\n",
    "    optim='adafactor',\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=False,\n",
    "\n",
    "    # Params for early stop\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 1024)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=1024, out_features=50304, bias=False)\n",
      ")\n",
      "Memory footprint 1.72829168 GB\n",
      "Flops 17391.09433344 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "    base_model.floating_point_ops(\n",
    "        {\n",
    "            'input_ids': torch.zeros(\n",
    "                (1, training_config['model']['mx_length']),\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    * training_args.gradient_accumulation_steps)\n",
    "\n",
    "print(base_model)\n",
    "print('Memory footprint', base_model.get_memory_footprint() / 1e9, 'GB')\n",
    "print('Flops', model_flops / 1e9, 'GFLOPs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=base_model,\n",
    "    # model_flops=model_flops,\n",
    "    # total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bef3bc7cbd4cae8854ab7dfc506471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5541, 'grad_norm': 38.851478576660156, 'learning_rate': 5e-06, 'epoch': 0.0}\n",
      "{'loss': 2.5024, 'grad_norm': 22.19594383239746, 'learning_rate': 1e-05, 'epoch': 0.01}\n",
      "{'loss': 3.1272, 'grad_norm': 26.380901336669922, 'learning_rate': 9.89795918367347e-06, 'epoch': 0.01}\n",
      "{'loss': 2.5447, 'grad_norm': 25.621217727661133, 'learning_rate': 9.795918367346939e-06, 'epoch': 0.01}\n",
      "{'loss': 2.437, 'grad_norm': 18.78028106689453, 'learning_rate': 9.693877551020408e-06, 'epoch': 0.02}\n",
      "{'loss': 2.2266, 'grad_norm': 25.858356475830078, 'learning_rate': 9.591836734693878e-06, 'epoch': 0.02}\n",
      "{'loss': 3.0474, 'grad_norm': 32.238948822021484, 'learning_rate': 9.489795918367348e-06, 'epoch': 0.02}\n",
      "{'loss': 2.2725, 'grad_norm': 22.35288429260254, 'learning_rate': 9.387755102040818e-06, 'epoch': 0.03}\n",
      "{'loss': 2.0118, 'grad_norm': 21.57244110107422, 'learning_rate': 9.285714285714288e-06, 'epoch': 0.03}\n",
      "{'loss': 2.3951, 'grad_norm': 26.357877731323242, 'learning_rate': 9.183673469387756e-06, 'epoch': 0.03}\n",
      "{'loss': 2.5095, 'grad_norm': 19.90812110900879, 'learning_rate': 9.081632653061225e-06, 'epoch': 0.03}\n",
      "{'loss': 2.3222, 'grad_norm': 27.822799682617188, 'learning_rate': 8.979591836734695e-06, 'epoch': 0.04}\n",
      "{'loss': 2.6833, 'grad_norm': 27.830495834350586, 'learning_rate': 8.877551020408163e-06, 'epoch': 0.04}\n",
      "{'loss': 1.6162, 'grad_norm': 21.48265266418457, 'learning_rate': 8.775510204081633e-06, 'epoch': 0.04}\n",
      "{'loss': 2.9455, 'grad_norm': 52.76475143432617, 'learning_rate': 8.673469387755103e-06, 'epoch': 0.05}\n",
      "{'loss': 1.9109, 'grad_norm': 18.71784019470215, 'learning_rate': 8.571428571428571e-06, 'epoch': 0.05}\n",
      "{'loss': 1.8409, 'grad_norm': 18.04155921936035, 'learning_rate': 8.469387755102042e-06, 'epoch': 0.05}\n",
      "{'loss': 2.3639, 'grad_norm': 33.474365234375, 'learning_rate': 8.36734693877551e-06, 'epoch': 0.06}\n",
      "{'loss': 1.8023, 'grad_norm': 20.026636123657227, 'learning_rate': 8.26530612244898e-06, 'epoch': 0.06}\n",
      "{'loss': 2.2434, 'grad_norm': 25.624210357666016, 'learning_rate': 8.16326530612245e-06, 'epoch': 0.06}\n",
      "{'loss': 2.4141, 'grad_norm': 20.160598754882812, 'learning_rate': 8.06122448979592e-06, 'epoch': 0.07}\n",
      "{'loss': 2.2934, 'grad_norm': 17.921375274658203, 'learning_rate': 7.959183673469388e-06, 'epoch': 0.07}\n",
      "{'loss': 1.6395, 'grad_norm': 20.01722526550293, 'learning_rate': 7.857142857142858e-06, 'epoch': 0.07}\n",
      "{'loss': 1.7716, 'grad_norm': 18.01880645751953, 'learning_rate': 7.755102040816327e-06, 'epoch': 0.08}\n",
      "{'loss': 1.4125, 'grad_norm': 17.23585319519043, 'learning_rate': 7.653061224489796e-06, 'epoch': 0.08}\n",
      "{'loss': 1.9612, 'grad_norm': 26.660615921020508, 'learning_rate': 7.551020408163265e-06, 'epoch': 0.08}\n",
      "{'loss': 1.887, 'grad_norm': 25.19187355041504, 'learning_rate': 7.448979591836736e-06, 'epoch': 0.09}\n",
      "{'loss': 1.6038, 'grad_norm': 15.921642303466797, 'learning_rate': 7.346938775510205e-06, 'epoch': 0.09}\n",
      "{'loss': 1.7767, 'grad_norm': 25.868179321289062, 'learning_rate': 7.244897959183675e-06, 'epoch': 0.09}\n",
      "{'loss': 1.527, 'grad_norm': 22.327796936035156, 'learning_rate': 7.1428571428571436e-06, 'epoch': 0.1}\n",
      "{'loss': 2.2303, 'grad_norm': 21.82513427734375, 'learning_rate': 7.0408163265306125e-06, 'epoch': 0.1}\n",
      "{'loss': 1.4581, 'grad_norm': 19.99237060546875, 'learning_rate': 6.938775510204082e-06, 'epoch': 0.1}\n",
      "{'loss': 1.9459, 'grad_norm': 31.27109146118164, 'learning_rate': 6.836734693877551e-06, 'epoch': 0.1}\n",
      "{'loss': 1.9227, 'grad_norm': 20.3419132232666, 'learning_rate': 6.734693877551021e-06, 'epoch': 0.11}\n",
      "{'loss': 2.1062, 'grad_norm': 18.332130432128906, 'learning_rate': 6.63265306122449e-06, 'epoch': 0.11}\n",
      "{'loss': 2.1452, 'grad_norm': 22.084487915039062, 'learning_rate': 6.530612244897959e-06, 'epoch': 0.11}\n",
      "{'loss': 1.5605, 'grad_norm': 20.20781707763672, 'learning_rate': 6.4285714285714295e-06, 'epoch': 0.12}\n",
      "{'loss': 2.0988, 'grad_norm': 25.389074325561523, 'learning_rate': 6.326530612244899e-06, 'epoch': 0.12}\n",
      "{'loss': 1.8298, 'grad_norm': 23.79808235168457, 'learning_rate': 6.224489795918368e-06, 'epoch': 0.12}\n",
      "{'loss': 2.1133, 'grad_norm': 24.305910110473633, 'learning_rate': 6.122448979591837e-06, 'epoch': 0.13}\n",
      "{'loss': 2.0196, 'grad_norm': 21.775297164916992, 'learning_rate': 6.020408163265307e-06, 'epoch': 0.13}\n",
      "{'loss': 1.3538, 'grad_norm': 17.790250778198242, 'learning_rate': 5.918367346938776e-06, 'epoch': 0.13}\n",
      "{'loss': 2.1076, 'grad_norm': 18.296445846557617, 'learning_rate': 5.816326530612246e-06, 'epoch': 0.14}\n",
      "{'loss': 2.3028, 'grad_norm': 17.84564781188965, 'learning_rate': 5.7142857142857145e-06, 'epoch': 0.14}\n",
      "{'loss': 1.5573, 'grad_norm': 15.712422370910645, 'learning_rate': 5.6122448979591834e-06, 'epoch': 0.14}\n",
      "{'loss': 1.985, 'grad_norm': 23.789228439331055, 'learning_rate': 5.510204081632653e-06, 'epoch': 0.15}\n",
      "{'loss': 2.1757, 'grad_norm': 24.12977409362793, 'learning_rate': 5.408163265306123e-06, 'epoch': 0.15}\n",
      "{'loss': 1.2535, 'grad_norm': 22.490942001342773, 'learning_rate': 5.306122448979593e-06, 'epoch': 0.15}\n",
      "{'loss': 1.8382, 'grad_norm': 24.076618194580078, 'learning_rate': 5.204081632653062e-06, 'epoch': 0.16}\n",
      "{'loss': 1.6459, 'grad_norm': 18.77154541015625, 'learning_rate': 5.1020408163265315e-06, 'epoch': 0.16}\n",
      "{'loss': 1.3478, 'grad_norm': 15.312970161437988, 'learning_rate': 5e-06, 'epoch': 0.16}\n",
      "{'loss': 1.8459, 'grad_norm': 15.863754272460938, 'learning_rate': 4.897959183673469e-06, 'epoch': 0.17}\n",
      "{'loss': 2.1039, 'grad_norm': 35.4766731262207, 'learning_rate': 4.795918367346939e-06, 'epoch': 0.17}\n",
      "{'loss': 1.9823, 'grad_norm': 22.771482467651367, 'learning_rate': 4.693877551020409e-06, 'epoch': 0.17}\n",
      "{'loss': 1.7794, 'grad_norm': 17.759174346923828, 'learning_rate': 4.591836734693878e-06, 'epoch': 0.17}\n",
      "{'loss': 2.1309, 'grad_norm': 17.241107940673828, 'learning_rate': 4.489795918367348e-06, 'epoch': 0.18}\n",
      "{'loss': 1.8523, 'grad_norm': 21.795198440551758, 'learning_rate': 4.3877551020408165e-06, 'epoch': 0.18}\n",
      "{'loss': 1.6291, 'grad_norm': 17.002161026000977, 'learning_rate': 4.2857142857142855e-06, 'epoch': 0.18}\n",
      "{'loss': 2.0918, 'grad_norm': 21.72397232055664, 'learning_rate': 4.183673469387755e-06, 'epoch': 0.19}\n",
      "{'loss': 1.2631, 'grad_norm': 18.361848831176758, 'learning_rate': 4.081632653061225e-06, 'epoch': 0.19}\n",
      "{'loss': 2.0782, 'grad_norm': 22.201068878173828, 'learning_rate': 3.979591836734694e-06, 'epoch': 0.19}\n",
      "{'loss': 1.8776, 'grad_norm': 23.936098098754883, 'learning_rate': 3.877551020408164e-06, 'epoch': 0.2}\n",
      "{'loss': 1.9591, 'grad_norm': 22.029359817504883, 'learning_rate': 3.7755102040816327e-06, 'epoch': 0.2}\n",
      "{'loss': 1.8503, 'grad_norm': 23.13005256652832, 'learning_rate': 3.6734693877551024e-06, 'epoch': 0.2}\n",
      "{'loss': 1.6367, 'grad_norm': 17.501441955566406, 'learning_rate': 3.5714285714285718e-06, 'epoch': 0.21}\n",
      "{'loss': 1.7075, 'grad_norm': 16.407772064208984, 'learning_rate': 3.469387755102041e-06, 'epoch': 0.21}\n",
      "{'loss': 2.2936, 'grad_norm': 20.501873016357422, 'learning_rate': 3.3673469387755105e-06, 'epoch': 0.21}\n",
      "{'loss': 1.3164, 'grad_norm': 22.498701095581055, 'learning_rate': 3.2653061224489794e-06, 'epoch': 0.22}\n",
      "{'loss': 2.3245, 'grad_norm': 31.974637985229492, 'learning_rate': 3.1632653061224496e-06, 'epoch': 0.22}\n",
      "{'loss': 1.2111, 'grad_norm': 15.910120964050293, 'learning_rate': 3.0612244897959185e-06, 'epoch': 0.22}\n",
      "{'loss': 1.9821, 'grad_norm': 18.772287368774414, 'learning_rate': 2.959183673469388e-06, 'epoch': 0.23}\n",
      "{'loss': 1.5703, 'grad_norm': 18.031408309936523, 'learning_rate': 2.8571428571428573e-06, 'epoch': 0.23}\n",
      "{'loss': 1.4147, 'grad_norm': 16.449373245239258, 'learning_rate': 2.7551020408163266e-06, 'epoch': 0.23}\n",
      "{'loss': 1.7156, 'grad_norm': 18.881240844726562, 'learning_rate': 2.6530612244897964e-06, 'epoch': 0.23}\n",
      "{'loss': 1.023, 'grad_norm': 14.304615020751953, 'learning_rate': 2.5510204081632657e-06, 'epoch': 0.24}\n",
      "{'loss': 1.7137, 'grad_norm': 16.869165420532227, 'learning_rate': 2.4489795918367347e-06, 'epoch': 0.24}\n",
      "{'loss': 1.7565, 'grad_norm': 20.027822494506836, 'learning_rate': 2.3469387755102044e-06, 'epoch': 0.24}\n",
      "{'loss': 2.2262, 'grad_norm': 37.80681610107422, 'learning_rate': 2.244897959183674e-06, 'epoch': 0.25}\n",
      "{'loss': 1.881, 'grad_norm': 20.207048416137695, 'learning_rate': 2.1428571428571427e-06, 'epoch': 0.25}\n",
      "{'loss': 1.9121, 'grad_norm': 17.35264778137207, 'learning_rate': 2.0408163265306125e-06, 'epoch': 0.25}\n",
      "{'loss': 2.3717, 'grad_norm': 19.589506149291992, 'learning_rate': 1.938775510204082e-06, 'epoch': 0.26}\n",
      "{'loss': 1.5871, 'grad_norm': 17.12220001220703, 'learning_rate': 1.8367346938775512e-06, 'epoch': 0.26}\n",
      "{'loss': 1.5013, 'grad_norm': 15.721407890319824, 'learning_rate': 1.7346938775510206e-06, 'epoch': 0.26}\n",
      "{'loss': 2.0655, 'grad_norm': 20.06834602355957, 'learning_rate': 1.6326530612244897e-06, 'epoch': 0.27}\n",
      "{'loss': 2.0652, 'grad_norm': 21.7208251953125, 'learning_rate': 1.5306122448979593e-06, 'epoch': 0.27}\n",
      "{'loss': 1.8475, 'grad_norm': 18.315523147583008, 'learning_rate': 1.4285714285714286e-06, 'epoch': 0.27}\n",
      "{'loss': 1.7888, 'grad_norm': 20.04651641845703, 'learning_rate': 1.3265306122448982e-06, 'epoch': 0.28}\n",
      "{'loss': 2.325, 'grad_norm': 22.42350196838379, 'learning_rate': 1.2244897959183673e-06, 'epoch': 0.28}\n",
      "{'loss': 1.7573, 'grad_norm': 14.838316917419434, 'learning_rate': 1.122448979591837e-06, 'epoch': 0.28}\n",
      "{'loss': 1.4708, 'grad_norm': 14.077346801757812, 'learning_rate': 1.0204081632653063e-06, 'epoch': 0.29}\n",
      "{'loss': 1.4364, 'grad_norm': 16.526105880737305, 'learning_rate': 9.183673469387756e-07, 'epoch': 0.29}\n",
      "{'loss': 2.1306, 'grad_norm': 16.5537166595459, 'learning_rate': 8.163265306122449e-07, 'epoch': 0.29}\n",
      "{'loss': 2.0509, 'grad_norm': 20.45064353942871, 'learning_rate': 7.142857142857143e-07, 'epoch': 0.3}\n",
      "{'loss': 2.151, 'grad_norm': 20.518522262573242, 'learning_rate': 6.122448979591837e-07, 'epoch': 0.3}\n",
      "{'loss': 1.7551, 'grad_norm': 17.059593200683594, 'learning_rate': 5.102040816326531e-07, 'epoch': 0.3}\n",
      "{'loss': 1.1687, 'grad_norm': 17.380271911621094, 'learning_rate': 4.0816326530612243e-07, 'epoch': 0.3}\n",
      "{'loss': 1.5306, 'grad_norm': 17.337919235229492, 'learning_rate': 3.0612244897959183e-07, 'epoch': 0.31}\n",
      "{'loss': 1.5976, 'grad_norm': 16.42569351196289, 'learning_rate': 2.0408163265306121e-07, 'epoch': 0.31}\n",
      "{'loss': 1.33, 'grad_norm': 14.57230281829834, 'learning_rate': 1.0204081632653061e-07, 'epoch': 0.31}\n",
      "{'loss': 2.0264, 'grad_norm': 16.558338165283203, 'learning_rate': 0.0, 'epoch': 0.32}\n",
      "{'train_runtime': 192.5415, 'train_samples_per_second': 2.077, 'train_steps_per_second': 0.519, 'train_loss': 1.9372756588459015, 'epoch': 0.32}\n"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to lamini_docs_100_steps/final\n"
     ]
    }
   ],
   "source": [
    "save_dir = f'{output_dir}/final'\n",
    "trainer.save_model(save_dir)\n",
    "print('Saved model to', save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 1024)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=1024, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
    "finetuned_slightly_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test):  Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Finetuned slightly model's answer: \n",
      "Yes, Lamini can generate technical documentation or user manuals for software projects. It can be used to train models and generate documentation for software projects. It can also be used to train models and generate documentation for hardware projects. Additionally, Lamini can be used to train models and generate documentation for data-intensive applications such as machine learning. Additionally, Lamini can be used to train models and generate documentation for\n"
     ]
    }
   ],
   "source": [
    "test_question = test_dataset[0]['question']\n",
    "print('Question input (test): ', test_question)\n",
    "print('Finetuned slightly model\\'s answer: ')\n",
    "print(inference(test_question, finetuned_slightly_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target answer:  Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n"
     ]
    }
   ],
   "source": [
    "print('Target answer: ', test_dataset[0]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
